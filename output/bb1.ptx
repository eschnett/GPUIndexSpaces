// PTX CompilerJob of kernel #runsteps(CuDeviceVector{Int8x4, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Int32, 1}, CuDeviceVector{Int4x8, 1}) for sm_86, minthreads=128, blocks_per_sm=8, always_inline=false

//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE // -- Begin function _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
// TODO.extern .func gpu_report_exception
// TODO(
// TODO	.param .b64 gpu_report_exception_param_0
// TODO)
// TODO;
.func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
.noreturn
{
        trap;
}

//TODO .extern .func gpu_signal_exception
//TODO (
//TODO 	.param .align 8 .b8 gpu_signal_exception_param_0[8]
//TODO )
//TODO ;
.func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[8]
)
.noreturn
{
        trap;
}

.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
.visible .entry _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE(
	.param .align 8 .b8 _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0[8],
	.param .align 8 .b8 _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4[32]
)
.reqntid 128, 1, 1
.minnctapersm 8
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<1086>;
	.reg .b64 	%rd<136>;

// %bb.0:                               // %conversion
	ld.param.u64 	%rd33, [_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0];
	// begin inline asm
	mov.u32 %r71, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r71, 4223;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	mov.u64 	%rd36, shmem;
	cvta.shared.u64 	%rd37, %rd36;
	cvta.to.shared.u64 	%rd38, %rd37;
	and.b64  	%rd39, %rd38, 127;
	setp.ne.s64 	%p2, %rd39, 0;
	@%p2 bra 	LBB0_12;
// %bb.3:                               // %L27
	// begin inline asm
	mov.u32 %r72, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p3, %r72, 9343;
	@%p3 bra 	LBB0_5;
	bra.uni 	LBB0_4;
LBB0_5:                                 // %L36
	add.s64 	%rd43, %rd36, 4224;
	and.b64  	%rd44, %rd43, 127;
	setp.ne.s64 	%p4, %rd44, 0;
	@%p4 bra 	LBB0_11;
// %bb.6:                               // %L50
	ld.param.u64 	%rd1, [_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1];
	ld.param.u64 	%rd2, [_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2];
	ld.param.u64 	%rd3, [_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3];
	ld.param.u64 	%rd4, [_Z19julia_runsteps_646013CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4];
	mov.u32 	%r122, %ctaid.x;
	shl.b32 	%r123, %r122, 5;
	and.b32  	%r124, %r123, 32736;
	mov.u32 	%r125, %tid.y;
	shl.b32 	%r126, %r125, 2;
	and.b32  	%r127, %r126, 12;
	mov.u32 	%r128, %tid.x;
	bfe.u32 	%r129, %r128, 3, 2;
	or.b32  	%r130, %r127, %r124;
	or.b32  	%r131, %r130, %r129;
	mul.wide.u32 	%rd45, %r131, 4;
	add.s64 	%rd46, %rd3, %rd45;
	ld.global.u32 	%r132, [%rd46];
	ld.global.u32 	%r133, [%rd46+64];
	shl.b32 	%r134, %r125, 8;
	and.b32  	%r135, %r134, 768;
	and.b32  	%r1, %r128, 4;
	shl.b32 	%r136, %r128, 3;
	and.b32  	%r137, %r136, 216;
	or.b32  	%r138, %r1, %r137;
	or.b32  	%r139, %r138, %r135;
	shl.b32 	%r140, %r139, 2;
	cvt.u64.u32 	%rd47, %r140;
	add.s64 	%rd48, %rd47, %rd1;
	ld.global.v4.u32 	{%r74, %r75, %r80, %r81}, [%rd48];
	ld.global.v4.u32 	{%r86, %r87, %r92, %r93}, [%rd48+128];
	// begin inline asm
	prmt.b32 %r98, %r74, %r75, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r99, %r74, %r75, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r104, %r80, %r81, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r105, %r80, %r81, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r110, %r86, %r87, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r111, %r86, %r87, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r116, %r92, %r93, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r117, %r92, %r93, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r97, %r98, %r99, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r100, %r98, %r99, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r103, %r104, %r105, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r106, %r104, %r105, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r109, %r110, %r111, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r112, %r110, %r111, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r115, %r116, %r117, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r118, %r116, %r117, 29521;
	// end inline asm
	setp.eq.s32 	%p5, %r1, 0;
	selp.b32 	%r141, %r109, %r97, %p5;
	shfl.sync.bfly.b32	%r142, %r141, 4, 31, -1;
	selp.b32 	%r143, %r112, %r100, %p5;
	selp.b32 	%r283, %r142, %r109, %p5;
	selp.b32 	%r219, %r97, %r142, %p5;
	shfl.sync.bfly.b32	%r144, %r143, 4, 31, -1;
	selp.b32 	%r145, %r115, %r103, %p5;
	selp.b32 	%r289, %r144, %r112, %p5;
	selp.b32 	%r225, %r100, %r144, %p5;
	shfl.sync.bfly.b32	%r146, %r145, 4, 31, -1;
	selp.b32 	%r147, %r118, %r106, %p5;
	selp.b32 	%r315, %r146, %r115, %p5;
	selp.b32 	%r251, %r103, %r146, %p5;
	shfl.sync.bfly.b32	%r148, %r147, 4, 31, -1;
	selp.b32 	%r321, %r148, %r118, %p5;
	selp.b32 	%r257, %r106, %r148, %p5;
	shl.b32 	%r149, %r122, 7;
	and.b32  	%r10, %r149, 130944;
	shl.b32 	%r150, %r125, 5;
	shl.b32 	%r151, %r125, 21;
	or.b32  	%r152, %r150, %r151;
	and.b32  	%r11, %r152, 2097216;
	shl.b32 	%r153, %r128, 4;
	and.b32  	%r12, %r153, 48;
	shl.b32 	%r154, %r128, 14;
	and.b32  	%r13, %r154, 393216;
	shl.b32 	%r155, %r128, 18;
	and.b32  	%r14, %r155, 1048576;
	shl.b32 	%r156, %r125, 3;
	and.b32  	%r157, %r156, 16;
	shl.b32 	%r158, %r128, 2;
	and.b32  	%r159, %r158, 12;
	or.b32  	%r160, %r157, %r159;
	shl.b32 	%r161, %r125, 4;
	and.b32  	%r162, %r161, 16;
	shl.b32 	%r163, %r128, 1;
	and.b32  	%r164, %r163, 8;
	or.b32  	%r165, %r129, %r164;
	or.b32  	%r166, %r165, %r162;
	mul.lo.s32 	%r167, %r166, 33;
	cvt.u64.u32 	%rd49, %r167;
	cvt.u64.u32 	%rd50, %r160;
	add.s64 	%rd51, %rd50, %rd49;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd54, %rd53, %rd36;
	add.s64 	%rd6, %rd54, -4;
	or.b32  	%r168, %r162, %r129;
	or.b32  	%r169, %r168, %r164;
	or.b32  	%r170, %r169, 4;
	mul.lo.s32 	%r171, %r170, 33;
	cvt.u64.u32 	%rd55, %r171;
	add.s64 	%rd56, %rd50, %rd55;
	shl.b64 	%rd57, %rd56, 2;
	add.s64 	%rd58, %rd57, %rd36;
	add.s64 	%rd7, %rd58, -4;
	bfe.u32 	%r172, %r128, 2, 3;
	mul.lo.s32 	%r173, %r172, 33;
	cvt.u64.u32 	%rd59, %r173;
	add.s64 	%rd60, %rd50, %rd59;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd62, %rd61, %rd36;
	add.s64 	%rd9, %rd62, -4;
	and.b32  	%r174, %r156, 8;
	and.b32  	%r175, %r163, 6;
	or.b32  	%r176, %r175, 1;
	bfe.u32 	%r177, %r125, 1, 1;
	mul.lo.s32 	%r178, %r176, 20;
	cvt.u64.u32 	%rd63, %r178;
	mul.lo.s32 	%r179, %r177, 640;
	cvt.u64.u32 	%rd64, %r179;
	or.b32  	%r180, %r174, %r172;
	cvt.u64.u32 	%rd65, %r180;
	add.s64 	%rd66, %rd65, -21;
	add.s64 	%rd67, %rd66, %rd63;
	add.s64 	%rd68, %rd67, %rd64;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd11, %rd43, %rd69;
	add.s64 	%rd71, %rd65, %rd63;
	add.s64 	%rd72, %rd71, %rd64;
	shl.b64 	%rd73, %rd72, 2;
	add.s64 	%rd74, %rd73, %rd43;
	add.s64 	%rd12, %rd74, -4;
	or.b32  	%r181, %r172, 8;
	mul.lo.s32 	%r182, %r181, 33;
	cvt.u64.u32 	%rd75, %r182;
	add.s64 	%rd76, %rd50, %rd75;
	shl.b64 	%rd77, %rd76, 2;
	add.s64 	%rd78, %rd77, %rd36;
	add.s64 	%rd13, %rd78, -4;
	or.b32  	%r183, %r175, 9;
	mul.lo.s32 	%r184, %r183, 20;
	cvt.u64.u32 	%rd79, %r184;
	add.s64 	%rd80, %rd66, %rd79;
	add.s64 	%rd81, %rd80, %rd64;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd15, %rd43, %rd82;
	add.s64 	%rd83, %rd65, %rd79;
	add.s64 	%rd84, %rd83, %rd64;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd86, %rd85, %rd43;
	add.s64 	%rd16, %rd86, -4;
	or.b32  	%r185, %r172, 16;
	mul.lo.s32 	%r186, %r185, 33;
	cvt.u64.u32 	%rd87, %r186;
	add.s64 	%rd88, %rd50, %rd87;
	shl.b64 	%rd89, %rd88, 2;
	add.s64 	%rd90, %rd89, %rd36;
	add.s64 	%rd17, %rd90, -4;
	or.b32  	%r187, %r175, 17;
	mul.lo.s32 	%r188, %r187, 20;
	cvt.u64.u32 	%rd91, %r188;
	add.s64 	%rd92, %rd66, %rd91;
	add.s64 	%rd93, %rd92, %rd64;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd19, %rd43, %rd94;
	add.s64 	%rd95, %rd65, %rd91;
	add.s64 	%rd96, %rd95, %rd64;
	shl.b64 	%rd97, %rd96, 2;
	add.s64 	%rd98, %rd97, %rd43;
	add.s64 	%rd20, %rd98, -4;
	or.b32  	%r189, %r172, 24;
	mul.lo.s32 	%r190, %r189, 33;
	cvt.u64.u32 	%rd99, %r190;
	add.s64 	%rd100, %rd50, %rd99;
	shl.b64 	%rd101, %rd100, 2;
	add.s64 	%rd102, %rd101, %rd36;
	add.s64 	%rd21, %rd102, -4;
	or.b32  	%r191, %r175, 25;
	mul.lo.s32 	%r192, %r191, 20;
	cvt.u64.u32 	%rd103, %r192;
	add.s64 	%rd104, %rd66, %rd103;
	add.s64 	%rd105, %rd104, %rd64;
	shl.b64 	%rd106, %rd105, 2;
	add.s64 	%rd23, %rd43, %rd106;
	add.s64 	%rd107, %rd65, %rd103;
	add.s64 	%rd108, %rd107, %rd64;
	shl.b64 	%rd109, %rd108, 2;
	add.s64 	%rd110, %rd109, %rd43;
	add.s64 	%rd24, %rd110, -4;
	and.b32  	%r193, %r128, 7;
	mul.lo.s32 	%r194, %r193, 20;
	cvt.u64.u32 	%rd111, %r194;
	or.b32  	%r195, %r127, %r129;
	cvt.u64.u32 	%rd112, %r195;
	add.s64 	%rd113, %rd112, %rd111;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd29, %rd43, %rd114;
	add.s64 	%rd25, %rd29, -4;
	add.s32 	%r196, %r194, 160;
	cvt.u64.u32 	%rd115, %r196;
	add.s64 	%rd116, %rd112, %rd115;
	shl.b64 	%rd117, %rd116, 2;
	add.s64 	%rd30, %rd43, %rd117;
	add.s64 	%rd26, %rd30, -4;
	add.s32 	%r197, %r194, 320;
	cvt.u64.u32 	%rd118, %r197;
	add.s64 	%rd119, %rd112, %rd118;
	shl.b64 	%rd120, %rd119, 2;
	add.s64 	%rd31, %rd43, %rd120;
	add.s64 	%rd27, %rd31, -4;
	add.s32 	%r198, %r194, 480;
	cvt.u64.u32 	%rd121, %r198;
	add.s64 	%rd122, %rd112, %rd121;
	shl.b64 	%rd123, %rd122, 2;
	add.s64 	%rd32, %rd43, %rd123;
	add.s64 	%rd28, %rd32, -4;
	add.s32 	%r199, %r132, -1;
	mov.u32 	%r200, 1;
	shl.b32 	%r201, %r200, %r199;
	setp.gt.u32 	%p6, %r199, 31;
	selp.b32 	%r15, 0, %r201, %p6;
	min.u32 	%r16, %r132, 31;
	add.s32 	%r202, %r133, -1;
	shl.b32 	%r203, %r200, %r202;
	setp.gt.u32 	%p7, %r202, 31;
	selp.b32 	%r17, 0, %r203, %p7;
	min.u32 	%r18, %r133, 31;
	and.b32  	%r19, %r128, 1;
	and.b32  	%r20, %r128, 2;
	shl.b32 	%r204, %r122, 16;
	and.b32  	%r21, %r204, 67043328;
	shl.b32 	%r205, %r125, 28;
	and.b32  	%r22, %r205, 805306368;
	and.b32  	%r23, %r136, 32;
	shl.b32 	%r206, %r128, 5;
	and.b32  	%r24, %r206, 64;
	shl.b32 	%r207, %r128, 23;
	and.b32  	%r25, %r207, 201326592;
	and.b32  	%r26, %r153, 16;
	mov.u32 	%r221, 0;
	mov.u32 	%r1059, %r221;
	mov.u32 	%r1060, %r221;
	mov.u32 	%r1074, %r221;
	mov.u32 	%r1075, %r221;
	mov.u32 	%r1076, %r221;
	mov.u32 	%r1077, %r221;
	mov.u32 	%r1078, %r221;
	mov.u32 	%r1079, %r221;
	mov.u32 	%r1080, %r221;
	mov.u32 	%r1081, %r221;
	mov.u32 	%r1082, %r221;
	mov.u32 	%r1083, %r221;
	mov.u32 	%r1084, %r221;
	mov.u32 	%r1085, %r221;
LBB0_7:                                 // %L657
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_8 Depth 2
	mov.u32 	%r1073, %r221;
LBB0_8:                                 // %L676
                                        //   Parent Loop BB0_7 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r781, %r1059, %r1073;
	add.s32 	%r782, %r781, 4;
	or.b32  	%r783, %r782, %r10;
	or.b32  	%r784, %r783, %r11;
	or.b32  	%r785, %r784, %r12;
	or.b32  	%r786, %r785, %r13;
	add.s32 	%r787, %r786, %r14;
	cvt.u64.u32 	%rd124, %r787;
	add.s64 	%rd125, %rd2, %rd124;
	ld.global.v4.u32 	{%r788, %r789, %r790, %r791}, [%rd125+-4];
	add.s32 	%r792, %r781, 524292;
	or.b32  	%r793, %r792, %r10;
	or.b32  	%r794, %r793, %r11;
	or.b32  	%r795, %r794, %r12;
	or.b32  	%r796, %r795, %r13;
	add.s32 	%r797, %r796, %r14;
	cvt.u64.u32 	%rd126, %r797;
	add.s64 	%rd127, %rd2, %rd126;
	ld.global.v4.u32 	{%r798, %r799, %r800, %r801}, [%rd127+-4];
	st.shared.u32 	[%rd6+4], %r788;
	st.shared.u32 	[%rd6+8], %r789;
	st.shared.u32 	[%rd6+12], %r790;
	st.shared.u32 	[%rd6+16], %r791;
	st.shared.u32 	[%rd7+4], %r798;
	st.shared.u32 	[%rd7+8], %r799;
	st.shared.u32 	[%rd7+12], %r800;
	st.shared.u32 	[%rd7+16], %r801;
	bar.sync 	0;
	ld.shared.u32 	%r210, [%rd9+4];
	mov.u32 	%r211, 134744072;
	mov.u32 	%r212, 252645135;
	// begin inline asm
	lop3.b32 %r209, %r210, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r802, %r209, 2021161080;
	xor.b32  	%r220, %r802, -2139062144;
	shr.u32 	%r214, %r210, 4;
	// begin inline asm
	lop3.b32 %r213, %r214, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r803, %r213, 2021161080;
	xor.b32  	%r226, %r803, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r217, %r218}, {%r219}, {%r220}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r223, %r224}, {%r225}, {%r226}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r229, %r230}, {%r219}, {%r226}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r235, %r236}, {%r225}, {%r220}, {%r229, %r230};
	// end inline asm
	ld.shared.u32 	%r242, [%rd9+8];
	// begin inline asm
	lop3.b32 %r241, %r242, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r804, %r241, 2021161080;
	xor.b32  	%r252, %r804, -2139062144;
	shr.u32 	%r246, %r242, 4;
	// begin inline asm
	lop3.b32 %r245, %r246, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r805, %r245, 2021161080;
	xor.b32  	%r258, %r805, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r249, %r250}, {%r251}, {%r252}, {%r217, %r218};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r255, %r256}, {%r257}, {%r258}, {%r223, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r261, %r262}, {%r251}, {%r258}, {%r235, %r236};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r267, %r268}, {%r257}, {%r252}, {%r261, %r262};
	// end inline asm
	ld.shared.u32 	%r274, [%rd9+12];
	// begin inline asm
	lop3.b32 %r273, %r274, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r806, %r273, 2021161080;
	xor.b32  	%r284, %r806, -2139062144;
	shr.u32 	%r278, %r274, 4;
	// begin inline asm
	lop3.b32 %r277, %r278, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r807, %r277, 2021161080;
	xor.b32  	%r290, %r807, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r281, %r282}, {%r283}, {%r284}, {%r249, %r250};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r287, %r288}, {%r289}, {%r290}, {%r255, %r256};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r293, %r294}, {%r283}, {%r290}, {%r267, %r268};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r299, %r300}, {%r289}, {%r284}, {%r293, %r294};
	// end inline asm
	ld.shared.u32 	%r306, [%rd9+16];
	// begin inline asm
	lop3.b32 %r305, %r306, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r808, %r305, 2021161080;
	xor.b32  	%r316, %r808, -2139062144;
	shr.u32 	%r310, %r306, 4;
	// begin inline asm
	lop3.b32 %r309, %r310, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r809, %r309, 2021161080;
	xor.b32  	%r322, %r809, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r313, %r314}, {%r315}, {%r316}, {%r281, %r282};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r319, %r320}, {%r321}, {%r322}, {%r287, %r288};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r325, %r326}, {%r315}, {%r322}, {%r299, %r300};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r331, %r332}, {%r321}, {%r316}, {%r325, %r326};
	// end inline asm
	sub.s32 	%r810, %r313, %r319;
	add.s32 	%r811, %r810, 2;
	shr.s32 	%r339, %r811, 2;
	sub.s32 	%r812, %r314, %r320;
	add.s32 	%r813, %r812, 2;
	shr.s32 	%r342, %r813, 2;
	add.s32 	%r814, %r331, 2;
	shr.s32 	%r338, %r814, 2;
	add.s32 	%r815, %r332, 2;
	shr.s32 	%r341, %r815, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r337, %r338, %r339;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r340, %r341, %r342;
	// end inline asm
	st.shared.u32 	[%rd11+4], %r337;
	st.shared.u32 	[%rd12+4], %r340;
	ld.shared.u32 	%r344, [%rd13+4];
	// begin inline asm
	lop3.b32 %r343, %r344, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r816, %r343, 2021161080;
	xor.b32  	%r354, %r816, -2139062144;
	shr.u32 	%r348, %r344, 4;
	// begin inline asm
	lop3.b32 %r347, %r348, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r817, %r347, 2021161080;
	xor.b32  	%r360, %r817, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r351, %r352}, {%r219}, {%r354}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r357, %r358}, {%r225}, {%r360}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r363, %r364}, {%r219}, {%r360}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r369, %r370}, {%r225}, {%r354}, {%r363, %r364};
	// end inline asm
	ld.shared.u32 	%r376, [%rd13+8];
	// begin inline asm
	lop3.b32 %r375, %r376, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r818, %r375, 2021161080;
	xor.b32  	%r386, %r818, -2139062144;
	shr.u32 	%r380, %r376, 4;
	// begin inline asm
	lop3.b32 %r379, %r380, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r819, %r379, 2021161080;
	xor.b32  	%r392, %r819, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r383, %r384}, {%r251}, {%r386}, {%r351, %r352};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r389, %r390}, {%r257}, {%r392}, {%r357, %r358};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r395, %r396}, {%r251}, {%r392}, {%r369, %r370};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r401, %r402}, {%r257}, {%r386}, {%r395, %r396};
	// end inline asm
	ld.shared.u32 	%r408, [%rd13+12];
	// begin inline asm
	lop3.b32 %r407, %r408, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r820, %r407, 2021161080;
	xor.b32  	%r418, %r820, -2139062144;
	shr.u32 	%r412, %r408, 4;
	// begin inline asm
	lop3.b32 %r411, %r412, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r821, %r411, 2021161080;
	xor.b32  	%r424, %r821, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r415, %r416}, {%r283}, {%r418}, {%r383, %r384};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r421, %r422}, {%r289}, {%r424}, {%r389, %r390};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r427, %r428}, {%r283}, {%r424}, {%r401, %r402};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r433, %r434}, {%r289}, {%r418}, {%r427, %r428};
	// end inline asm
	ld.shared.u32 	%r440, [%rd13+16];
	// begin inline asm
	lop3.b32 %r439, %r440, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r822, %r439, 2021161080;
	xor.b32  	%r450, %r822, -2139062144;
	shr.u32 	%r444, %r440, 4;
	// begin inline asm
	lop3.b32 %r443, %r444, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r823, %r443, 2021161080;
	xor.b32  	%r456, %r823, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r447, %r448}, {%r315}, {%r450}, {%r415, %r416};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r453, %r454}, {%r321}, {%r456}, {%r421, %r422};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r459, %r460}, {%r315}, {%r456}, {%r433, %r434};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r465, %r466}, {%r321}, {%r450}, {%r459, %r460};
	// end inline asm
	sub.s32 	%r824, %r447, %r453;
	add.s32 	%r825, %r824, 2;
	shr.s32 	%r473, %r825, 2;
	sub.s32 	%r826, %r448, %r454;
	add.s32 	%r827, %r826, 2;
	shr.s32 	%r476, %r827, 2;
	add.s32 	%r828, %r465, 2;
	shr.s32 	%r472, %r828, 2;
	add.s32 	%r829, %r466, 2;
	shr.s32 	%r475, %r829, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r471, %r472, %r473;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r474, %r475, %r476;
	// end inline asm
	st.shared.u32 	[%rd15+4], %r471;
	st.shared.u32 	[%rd16+4], %r474;
	ld.shared.u32 	%r478, [%rd17+4];
	// begin inline asm
	lop3.b32 %r477, %r478, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r830, %r477, 2021161080;
	xor.b32  	%r488, %r830, -2139062144;
	shr.u32 	%r482, %r478, 4;
	// begin inline asm
	lop3.b32 %r481, %r482, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r831, %r481, 2021161080;
	xor.b32  	%r494, %r831, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r485, %r486}, {%r219}, {%r488}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r491, %r492}, {%r225}, {%r494}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r497, %r498}, {%r219}, {%r494}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r503, %r504}, {%r225}, {%r488}, {%r497, %r498};
	// end inline asm
	ld.shared.u32 	%r510, [%rd17+8];
	// begin inline asm
	lop3.b32 %r509, %r510, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r832, %r509, 2021161080;
	xor.b32  	%r520, %r832, -2139062144;
	shr.u32 	%r514, %r510, 4;
	// begin inline asm
	lop3.b32 %r513, %r514, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r833, %r513, 2021161080;
	xor.b32  	%r526, %r833, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r517, %r518}, {%r251}, {%r520}, {%r485, %r486};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r523, %r524}, {%r257}, {%r526}, {%r491, %r492};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r529, %r530}, {%r251}, {%r526}, {%r503, %r504};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r535, %r536}, {%r257}, {%r520}, {%r529, %r530};
	// end inline asm
	ld.shared.u32 	%r542, [%rd17+12];
	// begin inline asm
	lop3.b32 %r541, %r542, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r834, %r541, 2021161080;
	xor.b32  	%r552, %r834, -2139062144;
	shr.u32 	%r546, %r542, 4;
	// begin inline asm
	lop3.b32 %r545, %r546, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r835, %r545, 2021161080;
	xor.b32  	%r558, %r835, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r549, %r550}, {%r283}, {%r552}, {%r517, %r518};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r555, %r556}, {%r289}, {%r558}, {%r523, %r524};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r561, %r562}, {%r283}, {%r558}, {%r535, %r536};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r567, %r568}, {%r289}, {%r552}, {%r561, %r562};
	// end inline asm
	ld.shared.u32 	%r574, [%rd17+16];
	// begin inline asm
	lop3.b32 %r573, %r574, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r836, %r573, 2021161080;
	xor.b32  	%r584, %r836, -2139062144;
	shr.u32 	%r578, %r574, 4;
	// begin inline asm
	lop3.b32 %r577, %r578, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r837, %r577, 2021161080;
	xor.b32  	%r590, %r837, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r581, %r582}, {%r315}, {%r584}, {%r549, %r550};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r587, %r588}, {%r321}, {%r590}, {%r555, %r556};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r593, %r594}, {%r315}, {%r590}, {%r567, %r568};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r599, %r600}, {%r321}, {%r584}, {%r593, %r594};
	// end inline asm
	sub.s32 	%r838, %r581, %r587;
	add.s32 	%r839, %r838, 2;
	shr.s32 	%r607, %r839, 2;
	sub.s32 	%r840, %r582, %r588;
	add.s32 	%r841, %r840, 2;
	shr.s32 	%r610, %r841, 2;
	add.s32 	%r842, %r599, 2;
	shr.s32 	%r606, %r842, 2;
	add.s32 	%r843, %r600, 2;
	shr.s32 	%r609, %r843, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r605, %r606, %r607;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r608, %r609, %r610;
	// end inline asm
	st.shared.u32 	[%rd19+4], %r605;
	st.shared.u32 	[%rd20+4], %r608;
	ld.shared.u32 	%r612, [%rd21+4];
	// begin inline asm
	lop3.b32 %r611, %r612, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r844, %r611, 2021161080;
	xor.b32  	%r622, %r844, -2139062144;
	shr.u32 	%r616, %r612, 4;
	// begin inline asm
	lop3.b32 %r615, %r616, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r845, %r615, 2021161080;
	xor.b32  	%r628, %r845, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r619, %r620}, {%r219}, {%r622}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r625, %r626}, {%r225}, {%r628}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r631, %r632}, {%r219}, {%r628}, {%r221, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r637, %r638}, {%r225}, {%r622}, {%r631, %r632};
	// end inline asm
	ld.shared.u32 	%r644, [%rd21+8];
	// begin inline asm
	lop3.b32 %r643, %r644, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r846, %r643, 2021161080;
	xor.b32  	%r654, %r846, -2139062144;
	shr.u32 	%r648, %r644, 4;
	// begin inline asm
	lop3.b32 %r647, %r648, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r847, %r647, 2021161080;
	xor.b32  	%r660, %r847, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r651, %r652}, {%r251}, {%r654}, {%r619, %r620};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r657, %r658}, {%r257}, {%r660}, {%r625, %r626};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r663, %r664}, {%r251}, {%r660}, {%r637, %r638};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r669, %r670}, {%r257}, {%r654}, {%r663, %r664};
	// end inline asm
	ld.shared.u32 	%r676, [%rd21+12];
	// begin inline asm
	lop3.b32 %r675, %r676, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r848, %r675, 2021161080;
	xor.b32  	%r686, %r848, -2139062144;
	shr.u32 	%r680, %r676, 4;
	// begin inline asm
	lop3.b32 %r679, %r680, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r849, %r679, 2021161080;
	xor.b32  	%r692, %r849, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r683, %r684}, {%r283}, {%r686}, {%r651, %r652};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r689, %r690}, {%r289}, {%r692}, {%r657, %r658};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r695, %r696}, {%r283}, {%r692}, {%r669, %r670};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r701, %r702}, {%r289}, {%r686}, {%r695, %r696};
	// end inline asm
	ld.shared.u32 	%r708, [%rd21+16];
	// begin inline asm
	lop3.b32 %r707, %r708, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r850, %r707, 2021161080;
	xor.b32  	%r718, %r850, -2139062144;
	shr.u32 	%r712, %r708, 4;
	// begin inline asm
	lop3.b32 %r711, %r712, %r211, %r212, 40;
	// end inline asm
	add.s32 	%r851, %r711, 2021161080;
	xor.b32  	%r724, %r851, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r715, %r716}, {%r315}, {%r718}, {%r683, %r684};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r721, %r722}, {%r321}, {%r724}, {%r689, %r690};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r727, %r728}, {%r315}, {%r724}, {%r701, %r702};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r733, %r734}, {%r321}, {%r718}, {%r727, %r728};
	// end inline asm
	sub.s32 	%r852, %r715, %r721;
	add.s32 	%r853, %r852, 2;
	shr.s32 	%r741, %r853, 2;
	sub.s32 	%r854, %r716, %r722;
	add.s32 	%r855, %r854, 2;
	shr.s32 	%r744, %r855, 2;
	add.s32 	%r856, %r733, 2;
	shr.s32 	%r740, %r856, 2;
	add.s32 	%r857, %r734, 2;
	shr.s32 	%r743, %r857, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r739, %r740, %r741;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r742, %r743, %r744;
	// end inline asm
	st.shared.u32 	[%rd23+4], %r739;
	st.shared.u32 	[%rd24+4], %r742;
	bar.sync 	0;
	ld.shared.u32 	%r858, [%rd25+4];
	ld.shared.u32 	%r859, [%rd26+4];
	ld.shared.u32 	%r860, [%rd27+4];
	ld.shared.u32 	%r861, [%rd28+4];
	ld.shared.u32 	%r862, [%rd29+2560];
	ld.shared.u32 	%r863, [%rd30+2560];
	ld.shared.u32 	%r864, [%rd31+2560];
	ld.shared.u32 	%r865, [%rd32+2560];
	cvt.s32.s16 	%r866, %r858;
	shr.s32 	%r867, %r858, 16;
	cvt.s32.s16 	%r868, %r859;
	shr.s32 	%r869, %r859, 16;
	cvt.s32.s16 	%r870, %r860;
	shr.s32 	%r871, %r860, 16;
	cvt.s32.s16 	%r872, %r861;
	shr.s32 	%r873, %r861, 16;
	cvt.s32.s16 	%r874, %r862;
	shr.s32 	%r875, %r862, 16;
	cvt.s32.s16 	%r876, %r863;
	shr.s32 	%r877, %r863, 16;
	cvt.s32.s16 	%r878, %r864;
	shr.s32 	%r879, %r864, 16;
	cvt.s32.s16 	%r880, %r865;
	shr.s32 	%r881, %r865, 16;
	add.s32 	%r882, %r866, %r15;
	shr.s32 	%r883, %r882, %r16;
	add.s32 	%r884, %r867, %r15;
	shr.s32 	%r885, %r884, %r16;
	add.s32 	%r886, %r868, %r15;
	shr.s32 	%r887, %r886, %r16;
	add.s32 	%r888, %r869, %r15;
	shr.s32 	%r889, %r888, %r16;
	add.s32 	%r890, %r870, %r15;
	shr.s32 	%r891, %r890, %r16;
	add.s32 	%r892, %r871, %r15;
	shr.s32 	%r893, %r892, %r16;
	add.s32 	%r894, %r872, %r15;
	shr.s32 	%r895, %r894, %r16;
	add.s32 	%r896, %r873, %r15;
	shr.s32 	%r897, %r896, %r16;
	add.s32 	%r898, %r874, %r17;
	shr.s32 	%r899, %r898, %r18;
	add.s32 	%r900, %r875, %r17;
	shr.s32 	%r901, %r900, %r18;
	add.s32 	%r902, %r876, %r17;
	shr.s32 	%r903, %r902, %r18;
	add.s32 	%r904, %r877, %r17;
	shr.s32 	%r905, %r904, %r18;
	add.s32 	%r906, %r878, %r17;
	shr.s32 	%r907, %r906, %r18;
	add.s32 	%r908, %r879, %r17;
	shr.s32 	%r909, %r908, %r18;
	add.s32 	%r910, %r880, %r17;
	shr.s32 	%r911, %r910, %r18;
	add.s32 	%r912, %r881, %r17;
	shr.s32 	%r913, %r912, %r18;
	max.s32 	%r914, %r883, -7;
	min.s32 	%r750, %r914, 7;
	max.s32 	%r915, %r885, -7;
	min.s32 	%r757, %r915, 7;
	max.s32 	%r916, %r887, -7;
	min.s32 	%r749, %r916, 7;
	max.s32 	%r917, %r889, -7;
	min.s32 	%r756, %r917, 7;
	max.s32 	%r918, %r891, -7;
	min.s32 	%r747, %r918, 7;
	max.s32 	%r919, %r893, -7;
	min.s32 	%r754, %r919, 7;
	max.s32 	%r920, %r895, -7;
	min.s32 	%r746, %r920, 7;
	max.s32 	%r921, %r897, -7;
	min.s32 	%r753, %r921, 7;
	max.s32 	%r922, %r899, -7;
	min.s32 	%r768, %r922, 7;
	max.s32 	%r923, %r901, -7;
	min.s32 	%r775, %r923, 7;
	max.s32 	%r924, %r903, -7;
	min.s32 	%r767, %r924, 7;
	max.s32 	%r925, %r905, -7;
	min.s32 	%r774, %r925, 7;
	max.s32 	%r926, %r907, -7;
	min.s32 	%r765, %r926, 7;
	max.s32 	%r927, %r909, -7;
	min.s32 	%r772, %r927, 7;
	max.s32 	%r928, %r911, -7;
	min.s32 	%r764, %r928, 7;
	max.s32 	%r929, %r913, -7;
	min.s32 	%r771, %r929, 7;
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r745, %r746, %r747, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r748, %r749, %r750, %r745;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r752, %r753, %r754, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r755, %r756, %r757, %r752;
	// end inline asm
	shl.b32 	%r760, %r755, 4;
	mov.u32 	%r762, -252645136;
	// begin inline asm
	lop3.b32 %r938, %r760, %r748, %r762, 228;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r763, %r764, %r765, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r766, %r767, %r768, %r763;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r770, %r771, %r772, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r773, %r774, %r775, %r770;
	// end inline asm
	shl.b32 	%r778, %r773, 4;
	// begin inline asm
	lop3.b32 %r950, %r778, %r766, %r762, 228;
	// end inline asm
	setp.eq.s32 	%p8, %r1073, 0;
	selp.b32 	%r1084, %r938, %r1084, %p8;
	selp.b32 	%r1085, %r938, %r1085, %p8;
	setp.eq.s32 	%p9, %r1073, 4194304;
	selp.b32 	%r1082, %r938, %r1082, %p9;
	selp.b32 	%r1083, %r938, %r1083, %p9;
	setp.eq.s32 	%p10, %r1073, 8388608;
	selp.b32 	%r1080, %r938, %r1080, %p10;
	selp.b32 	%r1081, %r938, %r1081, %p10;
	selp.b32 	%r1078, %r950, %r1078, %p8;
	selp.b32 	%r1079, %r950, %r1079, %p8;
	selp.b32 	%r1076, %r950, %r1076, %p9;
	selp.b32 	%r1077, %r950, %r1077, %p9;
	selp.b32 	%r1074, %r950, %r1074, %p10;
	selp.b32 	%r1075, %r950, %r1075, %p10;
	add.s32 	%r1073, %r1073, 4194304;
	setp.ne.s32 	%p11, %r1073, 16777216;
	@%p11 bra 	LBB0_8;
// %bb.9:                               // %L6672
                                        //   in Loop: Header=BB0_7 Depth=1
	setp.eq.s32 	%p12, %r20, 0;
	setp.eq.s32 	%p13, %r19, 0;
	// begin inline asm
	prmt.b32 %r930, %r1084, %r1082, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r933, %r1085, %r1083, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r936, %r1080, %r938, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r939, %r1081, %r938, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r942, %r1078, %r1076, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r945, %r1079, %r1077, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r948, %r1074, %r950, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r951, %r1075, %r950, 29521;
	// end inline asm
	selp.b32 	%r1002, %r933, %r930, %p13;
	shfl.sync.bfly.b32	%r1003, %r1002, 1, 31, -1;
	selp.b32 	%r956, %r1003, %r933, %p13;
	selp.b32 	%r955, %r930, %r1003, %p13;
	selp.b32 	%r1004, %r939, %r936, %p13;
	shfl.sync.bfly.b32	%r1005, %r1004, 1, 31, -1;
	selp.b32 	%r962, %r1005, %r939, %p13;
	selp.b32 	%r961, %r936, %r1005, %p13;
	selp.b32 	%r1006, %r945, %r942, %p13;
	shfl.sync.bfly.b32	%r1007, %r1006, 1, 31, -1;
	selp.b32 	%r968, %r1007, %r945, %p13;
	selp.b32 	%r967, %r942, %r1007, %p13;
	selp.b32 	%r1008, %r951, %r948, %p13;
	shfl.sync.bfly.b32	%r1009, %r1008, 1, 31, -1;
	selp.b32 	%r974, %r1009, %r951, %p13;
	selp.b32 	%r973, %r948, %r1009, %p13;
	// begin inline asm
	prmt.b32 %r954, %r955, %r956, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r957, %r955, %r956, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r960, %r961, %r962, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r963, %r961, %r962, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r966, %r967, %r968, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r969, %r967, %r968, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r972, %r973, %r974, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r975, %r973, %r974, 29521;
	// end inline asm
	selp.b32 	%r1010, %r960, %r954, %p12;
	shfl.sync.bfly.b32	%r1011, %r1010, 2, 31, -1;
	selp.b32 	%r980, %r1011, %r960, %p12;
	selp.b32 	%r979, %r954, %r1011, %p12;
	selp.b32 	%r1012, %r963, %r957, %p12;
	shfl.sync.bfly.b32	%r1013, %r1012, 2, 31, -1;
	selp.b32 	%r986, %r1013, %r963, %p12;
	selp.b32 	%r985, %r957, %r1013, %p12;
	selp.b32 	%r1014, %r972, %r966, %p12;
	shfl.sync.bfly.b32	%r1015, %r1014, 2, 31, -1;
	selp.b32 	%r992, %r1015, %r972, %p12;
	selp.b32 	%r991, %r966, %r1015, %p12;
	selp.b32 	%r1016, %r975, %r969, %p12;
	shfl.sync.bfly.b32	%r1017, %r1016, 2, 31, -1;
	selp.b32 	%r998, %r1017, %r975, %p12;
	selp.b32 	%r997, %r969, %r1017, %p12;
	// begin inline asm
	prmt.b32 %r978, %r979, %r980, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r981, %r979, %r980, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r984, %r985, %r986, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r987, %r985, %r986, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r990, %r991, %r992, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r993, %r991, %r992, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r996, %r997, %r998, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r999, %r997, %r998, 30258;
	// end inline asm
	selp.b32 	%r1018, %r984, %r978, %p5;
	shfl.sync.bfly.b32	%r1019, %r1018, 4, 31, -1;
	selp.b32 	%r1020, %r1019, %r984, %p5;
	selp.b32 	%r1021, %r978, %r1019, %p5;
	selp.b32 	%r1022, %r987, %r981, %p5;
	shfl.sync.bfly.b32	%r1023, %r1022, 4, 31, -1;
	selp.b32 	%r1024, %r1023, %r987, %p5;
	selp.b32 	%r1025, %r981, %r1023, %p5;
	selp.b32 	%r1026, %r996, %r990, %p5;
	shfl.sync.bfly.b32	%r1027, %r1026, 4, 31, -1;
	selp.b32 	%r1028, %r1027, %r996, %p5;
	selp.b32 	%r1029, %r990, %r1027, %p5;
	selp.b32 	%r1030, %r999, %r993, %p5;
	shfl.sync.bfly.b32	%r1031, %r1030, 4, 31, -1;
	selp.b32 	%r1032, %r1031, %r999, %p5;
	selp.b32 	%r1033, %r993, %r1031, %p5;
	selp.b32 	%r1034, %r1025, %r1021, %p13;
	shfl.sync.bfly.b32	%r1035, %r1034, 1, 31, -1;
	selp.b32 	%r1036, %r1035, %r1025, %p13;
	selp.b32 	%r1037, %r1021, %r1035, %p13;
	selp.b32 	%r1038, %r1024, %r1020, %p13;
	shfl.sync.bfly.b32	%r1039, %r1038, 1, 31, -1;
	selp.b32 	%r1040, %r1039, %r1024, %p13;
	selp.b32 	%r1041, %r1020, %r1039, %p13;
	selp.b32 	%r1042, %r1033, %r1029, %p13;
	shfl.sync.bfly.b32	%r1043, %r1042, 1, 31, -1;
	selp.b32 	%r1044, %r1043, %r1033, %p13;
	selp.b32 	%r1045, %r1029, %r1043, %p13;
	selp.b32 	%r1046, %r1032, %r1028, %p13;
	shfl.sync.bfly.b32	%r1047, %r1046, 1, 31, -1;
	selp.b32 	%r1048, %r1047, %r1032, %p13;
	selp.b32 	%r1049, %r1028, %r1047, %p13;
	shl.b32 	%r1050, %r1060, 7;
	or.b32  	%r1051, %r1050, %r21;
	or.b32  	%r1052, %r1051, %r22;
	or.b32  	%r1053, %r1052, %r23;
	or.b32  	%r1054, %r1053, %r26;
	or.b32  	%r1055, %r1054, %r24;
	or.b32  	%r1056, %r1055, %r25;
	or.b32  	%r1057, %r1056, 4;
	cvt.u64.u32 	%rd128, %r1057;
	add.s64 	%rd129, %rd4, %rd128;
	st.global.v4.u32 	[%rd129+-4], {%r1037, %r1041, %r1036, %r1040};
	or.b32  	%r1058, %r1056, 32772;
	cvt.u64.u32 	%rd130, %r1058;
	add.s64 	%rd131, %rd4, %rd130;
	st.global.v4.u32 	[%rd131+-4], {%r1045, %r1049, %r1044, %r1048};
	add.s32 	%r69, %r1060, 1;
	add.s32 	%r1059, %r1059, 16777216;
	setp.ne.s32 	%p15, %r1060, 255;
	mov.u32 	%r1060, %r69;
	@%p15 bra 	LBB0_7;
// %bb.10:                              // %L7236
	ret;
LBB0_1:                                 // %L9
	mov.u64 	%rd34, exception1;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 0
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 1
	// begin inline asm
	exit;
	// end inline asm
LBB0_12:                                // %L7240
	mov.u64 	%rd134, exception1;
	cvta.global.u64 	%rd135, %rd134;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd135;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 6
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 7
	// begin inline asm
	exit;
	// end inline asm
LBB0_4:                                 // %L34
	mov.u64 	%rd40, exception1;
	cvta.global.u64 	%rd41, %rd40;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 2
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 3
	// begin inline asm
	exit;
	// end inline asm
LBB0_11:                                // %L7237
	mov.u64 	%rd132, exception1;
	cvta.global.u64 	%rd133, %rd132;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd133;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}

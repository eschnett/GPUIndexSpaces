// PTX CompilerJob of kernel #runsteps(CuDeviceVector{Int8x4, 1}, CuDeviceVector{Int4x8, 1}, CuDeviceVector{Int32, 1}, CuDeviceVector{Int4x8, 1}) for sm_86, minthreads=128, blocks_per_sm=8, always_inline=false

//
// Generated by LLVM NVPTX Back-End
//

.version 7.1
.target sm_86
.address_size 64

	// .globl	_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE // -- Begin function _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
.extern .func gpu_report_exception
(
	.param .b64 gpu_report_exception_param_0
)
;
.extern .func gpu_signal_exception
(
	.param .align 8 .b8 gpu_signal_exception_param_0[8]
)
;
.extern .shared .align 32 .b8 shmem[];
.global .align 1 .b8 exception1[10] = {101, 120, 99, 101, 112, 116, 105, 111, 110, 0};
                                        // @_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE
.visible .entry _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE(
	.param .align 8 .b8 _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0[8],
	.param .align 8 .b8 _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1[32],
	.param .align 8 .b8 _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2[32],
	.param .align 8 .b8 _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3[32],
	.param .align 8 .b8 _Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4[32]
)
.reqntid 128, 1, 1
.minnctapersm 8
{
	.reg .pred 	%p<16>;
	.reg .b32 	%r<1089>;
	.reg .b64 	%rd<136>;

// %bb.0:                               // %conversion
	ld.param.u64 	%rd33, [_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_0];
	// begin inline asm
	mov.u32 %r71, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p1, %r71, 4223;
	@%p1 bra 	LBB0_2;
	bra.uni 	LBB0_1;
LBB0_2:                                 // %L11
	mov.u64 	%rd36, shmem;
	cvta.shared.u64 	%rd37, %rd36;
	cvta.to.shared.u64 	%rd38, %rd37;
	and.b64  	%rd39, %rd38, 127;
	setp.ne.s64 	%p2, %rd39, 0;
	@%p2 bra 	LBB0_12;
// %bb.3:                               // %L27
	// begin inline asm
	mov.u32 %r72, %dynamic_smem_size;
	// end inline asm
	setp.gt.u32 	%p3, %r72, 9343;
	@%p3 bra 	LBB0_5;
	bra.uni 	LBB0_4;
LBB0_5:                                 // %L36
	add.s64 	%rd43, %rd36, 4224;
	and.b64  	%rd44, %rd43, 127;
	setp.ne.s64 	%p4, %rd44, 0;
	@%p4 bra 	LBB0_11;
// %bb.6:                               // %L50
	ld.param.u64 	%rd1, [_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_1];
	ld.param.u64 	%rd2, [_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_2];
	ld.param.u64 	%rd3, [_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_3];
	ld.param.u64 	%rd4, [_Z19julia_runsteps_644513CuDeviceArrayI6Int8x4Li1ELi1EES_I6Int4x8Li1ELi1EES_I5Int32Li1ELi1EES_IS1_Li1ELi1EE_param_4];
	mov.u32 	%r122, %ctaid.x;
	shl.b32 	%r123, %r122, 5;
	and.b32  	%r124, %r123, 32736;
	mov.u32 	%r125, %tid.y;
	shl.b32 	%r126, %r125, 2;
	and.b32  	%r127, %r126, 12;
	mov.u32 	%r128, %tid.x;
	bfe.u32 	%r129, %r128, 3, 2;
	or.b32  	%r130, %r127, %r124;
	or.b32  	%r131, %r130, %r129;
	mul.wide.u32 	%rd45, %r131, 4;
	add.s64 	%rd46, %rd3, %rd45;
	ld.global.u32 	%r132, [%rd46];
	ld.global.u32 	%r133, [%rd46+64];
	shl.b32 	%r134, %r122, 10;
	and.b32  	%r135, %r134, 1047552;
	shl.b32 	%r136, %r125, 8;
	and.b32  	%r137, %r136, 768;
	and.b32  	%r1, %r128, 4;
	shl.b32 	%r138, %r128, 3;
	and.b32  	%r139, %r138, 216;
	or.b32  	%r140, %r1, %r139;
	or.b32  	%r141, %r135, %r137;
	or.b32  	%r142, %r140, %r141;
	shl.b32 	%r143, %r142, 2;
	cvt.u64.u32 	%rd47, %r143;
	add.s64 	%rd48, %rd47, %rd1;
	ld.global.v4.u32 	{%r74, %r75, %r80, %r81}, [%rd48];
	ld.global.v4.u32 	{%r86, %r87, %r92, %r93}, [%rd48+128];
	// begin inline asm
	prmt.b32 %r98, %r74, %r75, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r99, %r74, %r75, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r104, %r80, %r81, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r105, %r80, %r81, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r110, %r86, %r87, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r111, %r86, %r87, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r116, %r92, %r93, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r117, %r92, %r93, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r97, %r98, %r99, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r100, %r98, %r99, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r103, %r104, %r105, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r106, %r104, %r105, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r109, %r110, %r111, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r112, %r110, %r111, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r115, %r116, %r117, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r118, %r116, %r117, 29521;
	// end inline asm
	setp.eq.s32 	%p5, %r1, 0;
	selp.b32 	%r144, %r109, %r97, %p5;
	shfl.sync.bfly.b32	%r145, %r144, 4, 31, -1;
	selp.b32 	%r146, %r112, %r100, %p5;
	selp.b32 	%r286, %r145, %r109, %p5;
	selp.b32 	%r222, %r97, %r145, %p5;
	shfl.sync.bfly.b32	%r147, %r146, 4, 31, -1;
	selp.b32 	%r148, %r115, %r103, %p5;
	selp.b32 	%r292, %r147, %r112, %p5;
	selp.b32 	%r228, %r100, %r147, %p5;
	shfl.sync.bfly.b32	%r149, %r148, 4, 31, -1;
	selp.b32 	%r150, %r118, %r106, %p5;
	selp.b32 	%r318, %r149, %r115, %p5;
	selp.b32 	%r254, %r103, %r149, %p5;
	shfl.sync.bfly.b32	%r151, %r150, 4, 31, -1;
	selp.b32 	%r324, %r151, %r118, %p5;
	selp.b32 	%r260, %r106, %r151, %p5;
	shl.b32 	%r152, %r122, 7;
	and.b32  	%r10, %r152, 130944;
	shl.b32 	%r153, %r125, 5;
	shl.b32 	%r154, %r125, 21;
	or.b32  	%r155, %r153, %r154;
	and.b32  	%r11, %r155, 2097216;
	shl.b32 	%r156, %r128, 4;
	and.b32  	%r12, %r156, 48;
	shl.b32 	%r157, %r128, 14;
	and.b32  	%r13, %r157, 393216;
	shl.b32 	%r158, %r128, 18;
	and.b32  	%r14, %r158, 1048576;
	shl.b32 	%r159, %r125, 3;
	and.b32  	%r160, %r159, 16;
	shl.b32 	%r161, %r128, 2;
	and.b32  	%r162, %r161, 12;
	or.b32  	%r163, %r160, %r162;
	shl.b32 	%r164, %r125, 4;
	and.b32  	%r165, %r164, 16;
	shl.b32 	%r166, %r128, 1;
	and.b32  	%r167, %r166, 8;
	or.b32  	%r168, %r129, %r167;
	or.b32  	%r169, %r168, %r165;
	mul.lo.s32 	%r170, %r169, 33;
	cvt.u64.u32 	%rd49, %r170;
	cvt.u64.u32 	%rd50, %r163;
	add.s64 	%rd51, %rd50, %rd49;
	shl.b64 	%rd53, %rd51, 2;
	add.s64 	%rd54, %rd53, %rd36;
	add.s64 	%rd6, %rd54, -4;
	or.b32  	%r171, %r165, %r129;
	or.b32  	%r172, %r171, %r167;
	or.b32  	%r173, %r172, 4;
	mul.lo.s32 	%r174, %r173, 33;
	cvt.u64.u32 	%rd55, %r174;
	add.s64 	%rd56, %rd50, %rd55;
	shl.b64 	%rd57, %rd56, 2;
	add.s64 	%rd58, %rd57, %rd36;
	add.s64 	%rd7, %rd58, -4;
	bfe.u32 	%r175, %r128, 2, 3;
	mul.lo.s32 	%r176, %r175, 33;
	cvt.u64.u32 	%rd59, %r176;
	add.s64 	%rd60, %rd50, %rd59;
	shl.b64 	%rd61, %rd60, 2;
	add.s64 	%rd62, %rd61, %rd36;
	add.s64 	%rd9, %rd62, -4;
	and.b32  	%r177, %r159, 8;
	and.b32  	%r178, %r166, 6;
	or.b32  	%r179, %r178, 1;
	bfe.u32 	%r180, %r125, 1, 1;
	mul.lo.s32 	%r181, %r179, 20;
	cvt.u64.u32 	%rd63, %r181;
	mul.lo.s32 	%r182, %r180, 640;
	cvt.u64.u32 	%rd64, %r182;
	or.b32  	%r183, %r177, %r175;
	cvt.u64.u32 	%rd65, %r183;
	add.s64 	%rd66, %rd65, -21;
	add.s64 	%rd67, %rd66, %rd63;
	add.s64 	%rd68, %rd67, %rd64;
	shl.b64 	%rd69, %rd68, 2;
	add.s64 	%rd11, %rd43, %rd69;
	add.s64 	%rd71, %rd65, %rd63;
	add.s64 	%rd72, %rd71, %rd64;
	shl.b64 	%rd73, %rd72, 2;
	add.s64 	%rd74, %rd73, %rd43;
	add.s64 	%rd12, %rd74, -4;
	or.b32  	%r184, %r175, 8;
	mul.lo.s32 	%r185, %r184, 33;
	cvt.u64.u32 	%rd75, %r185;
	add.s64 	%rd76, %rd50, %rd75;
	shl.b64 	%rd77, %rd76, 2;
	add.s64 	%rd78, %rd77, %rd36;
	add.s64 	%rd13, %rd78, -4;
	or.b32  	%r186, %r178, 9;
	mul.lo.s32 	%r187, %r186, 20;
	cvt.u64.u32 	%rd79, %r187;
	add.s64 	%rd80, %rd66, %rd79;
	add.s64 	%rd81, %rd80, %rd64;
	shl.b64 	%rd82, %rd81, 2;
	add.s64 	%rd15, %rd43, %rd82;
	add.s64 	%rd83, %rd65, %rd79;
	add.s64 	%rd84, %rd83, %rd64;
	shl.b64 	%rd85, %rd84, 2;
	add.s64 	%rd86, %rd85, %rd43;
	add.s64 	%rd16, %rd86, -4;
	or.b32  	%r188, %r175, 16;
	mul.lo.s32 	%r189, %r188, 33;
	cvt.u64.u32 	%rd87, %r189;
	add.s64 	%rd88, %rd50, %rd87;
	shl.b64 	%rd89, %rd88, 2;
	add.s64 	%rd90, %rd89, %rd36;
	add.s64 	%rd17, %rd90, -4;
	or.b32  	%r190, %r178, 17;
	mul.lo.s32 	%r191, %r190, 20;
	cvt.u64.u32 	%rd91, %r191;
	add.s64 	%rd92, %rd66, %rd91;
	add.s64 	%rd93, %rd92, %rd64;
	shl.b64 	%rd94, %rd93, 2;
	add.s64 	%rd19, %rd43, %rd94;
	add.s64 	%rd95, %rd65, %rd91;
	add.s64 	%rd96, %rd95, %rd64;
	shl.b64 	%rd97, %rd96, 2;
	add.s64 	%rd98, %rd97, %rd43;
	add.s64 	%rd20, %rd98, -4;
	or.b32  	%r192, %r175, 24;
	mul.lo.s32 	%r193, %r192, 33;
	cvt.u64.u32 	%rd99, %r193;
	add.s64 	%rd100, %rd50, %rd99;
	shl.b64 	%rd101, %rd100, 2;
	add.s64 	%rd102, %rd101, %rd36;
	add.s64 	%rd21, %rd102, -4;
	or.b32  	%r194, %r178, 25;
	mul.lo.s32 	%r195, %r194, 20;
	cvt.u64.u32 	%rd103, %r195;
	add.s64 	%rd104, %rd66, %rd103;
	add.s64 	%rd105, %rd104, %rd64;
	shl.b64 	%rd106, %rd105, 2;
	add.s64 	%rd23, %rd43, %rd106;
	add.s64 	%rd107, %rd65, %rd103;
	add.s64 	%rd108, %rd107, %rd64;
	shl.b64 	%rd109, %rd108, 2;
	add.s64 	%rd110, %rd109, %rd43;
	add.s64 	%rd24, %rd110, -4;
	and.b32  	%r196, %r128, 7;
	mul.lo.s32 	%r197, %r196, 20;
	cvt.u64.u32 	%rd111, %r197;
	or.b32  	%r198, %r127, %r129;
	cvt.u64.u32 	%rd112, %r198;
	add.s64 	%rd113, %rd112, %rd111;
	shl.b64 	%rd114, %rd113, 2;
	add.s64 	%rd29, %rd43, %rd114;
	add.s64 	%rd25, %rd29, -4;
	add.s32 	%r199, %r197, 160;
	cvt.u64.u32 	%rd115, %r199;
	add.s64 	%rd116, %rd112, %rd115;
	shl.b64 	%rd117, %rd116, 2;
	add.s64 	%rd30, %rd43, %rd117;
	add.s64 	%rd26, %rd30, -4;
	add.s32 	%r200, %r197, 320;
	cvt.u64.u32 	%rd118, %r200;
	add.s64 	%rd119, %rd112, %rd118;
	shl.b64 	%rd120, %rd119, 2;
	add.s64 	%rd31, %rd43, %rd120;
	add.s64 	%rd27, %rd31, -4;
	add.s32 	%r201, %r197, 480;
	cvt.u64.u32 	%rd121, %r201;
	add.s64 	%rd122, %rd112, %rd121;
	shl.b64 	%rd123, %rd122, 2;
	add.s64 	%rd32, %rd43, %rd123;
	add.s64 	%rd28, %rd32, -4;
	add.s32 	%r202, %r132, -1;
	mov.u32 	%r203, 1;
	shl.b32 	%r204, %r203, %r202;
	setp.gt.u32 	%p6, %r202, 31;
	selp.b32 	%r15, 0, %r204, %p6;
	min.u32 	%r16, %r132, 31;
	add.s32 	%r205, %r133, -1;
	shl.b32 	%r206, %r203, %r205;
	setp.gt.u32 	%p7, %r205, 31;
	selp.b32 	%r17, 0, %r206, %p7;
	min.u32 	%r18, %r133, 31;
	and.b32  	%r19, %r128, 1;
	and.b32  	%r20, %r128, 2;
	shl.b32 	%r207, %r122, 16;
	and.b32  	%r21, %r207, 67043328;
	shl.b32 	%r208, %r125, 28;
	and.b32  	%r22, %r208, 805306368;
	and.b32  	%r23, %r138, 32;
	shl.b32 	%r209, %r128, 5;
	and.b32  	%r24, %r209, 64;
	shl.b32 	%r210, %r128, 23;
	and.b32  	%r25, %r210, 201326592;
	and.b32  	%r26, %r156, 16;
	mov.u32 	%r224, 0;
	mov.u32 	%r1062, %r224;
	mov.u32 	%r1063, %r224;
	mov.u32 	%r1077, %r224;
	mov.u32 	%r1078, %r224;
	mov.u32 	%r1079, %r224;
	mov.u32 	%r1080, %r224;
	mov.u32 	%r1081, %r224;
	mov.u32 	%r1082, %r224;
	mov.u32 	%r1083, %r224;
	mov.u32 	%r1084, %r224;
	mov.u32 	%r1085, %r224;
	mov.u32 	%r1086, %r224;
	mov.u32 	%r1087, %r224;
	mov.u32 	%r1088, %r224;
LBB0_7:                                 // %L685
                                        // =>This Loop Header: Depth=1
                                        //     Child Loop BB0_8 Depth 2
	mov.u32 	%r1076, %r224;
LBB0_8:                                 // %L704
                                        //   Parent Loop BB0_7 Depth=1
                                        // =>  This Inner Loop Header: Depth=2
	add.s32 	%r784, %r1062, %r1076;
	add.s32 	%r785, %r784, 4;
	or.b32  	%r786, %r785, %r10;
	or.b32  	%r787, %r786, %r11;
	or.b32  	%r788, %r787, %r12;
	or.b32  	%r789, %r788, %r13;
	add.s32 	%r790, %r789, %r14;
	cvt.u64.u32 	%rd124, %r790;
	add.s64 	%rd125, %rd2, %rd124;
	ld.global.v4.u32 	{%r791, %r792, %r793, %r794}, [%rd125+-4];
	add.s32 	%r795, %r784, 524292;
	or.b32  	%r796, %r795, %r10;
	or.b32  	%r797, %r796, %r11;
	or.b32  	%r798, %r797, %r12;
	or.b32  	%r799, %r798, %r13;
	add.s32 	%r800, %r799, %r14;
	cvt.u64.u32 	%rd126, %r800;
	add.s64 	%rd127, %rd2, %rd126;
	ld.global.v4.u32 	{%r801, %r802, %r803, %r804}, [%rd127+-4];
	st.shared.u32 	[%rd6+4], %r791;
	st.shared.u32 	[%rd6+8], %r792;
	st.shared.u32 	[%rd6+12], %r793;
	st.shared.u32 	[%rd6+16], %r794;
	st.shared.u32 	[%rd7+4], %r801;
	st.shared.u32 	[%rd7+8], %r802;
	st.shared.u32 	[%rd7+12], %r803;
	st.shared.u32 	[%rd7+16], %r804;
	bar.sync 	0;
	ld.shared.u32 	%r213, [%rd9+4];
	mov.u32 	%r214, 134744072;
	mov.u32 	%r215, 252645135;
	// begin inline asm
	lop3.b32 %r212, %r213, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r805, %r212, 2021161080;
	xor.b32  	%r223, %r805, -2139062144;
	shr.u32 	%r217, %r213, 4;
	// begin inline asm
	lop3.b32 %r216, %r217, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r806, %r216, 2021161080;
	xor.b32  	%r229, %r806, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r220, %r221}, {%r222}, {%r223}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r226, %r227}, {%r228}, {%r229}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r232, %r233}, {%r222}, {%r229}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r238, %r239}, {%r228}, {%r223}, {%r232, %r233};
	// end inline asm
	ld.shared.u32 	%r245, [%rd9+8];
	// begin inline asm
	lop3.b32 %r244, %r245, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r807, %r244, 2021161080;
	xor.b32  	%r255, %r807, -2139062144;
	shr.u32 	%r249, %r245, 4;
	// begin inline asm
	lop3.b32 %r248, %r249, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r808, %r248, 2021161080;
	xor.b32  	%r261, %r808, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r252, %r253}, {%r254}, {%r255}, {%r220, %r221};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r258, %r259}, {%r260}, {%r261}, {%r226, %r227};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r264, %r265}, {%r254}, {%r261}, {%r238, %r239};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r270, %r271}, {%r260}, {%r255}, {%r264, %r265};
	// end inline asm
	ld.shared.u32 	%r277, [%rd9+12];
	// begin inline asm
	lop3.b32 %r276, %r277, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r809, %r276, 2021161080;
	xor.b32  	%r287, %r809, -2139062144;
	shr.u32 	%r281, %r277, 4;
	// begin inline asm
	lop3.b32 %r280, %r281, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r810, %r280, 2021161080;
	xor.b32  	%r293, %r810, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r284, %r285}, {%r286}, {%r287}, {%r252, %r253};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r290, %r291}, {%r292}, {%r293}, {%r258, %r259};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r296, %r297}, {%r286}, {%r293}, {%r270, %r271};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r302, %r303}, {%r292}, {%r287}, {%r296, %r297};
	// end inline asm
	ld.shared.u32 	%r309, [%rd9+16];
	// begin inline asm
	lop3.b32 %r308, %r309, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r811, %r308, 2021161080;
	xor.b32  	%r319, %r811, -2139062144;
	shr.u32 	%r313, %r309, 4;
	// begin inline asm
	lop3.b32 %r312, %r313, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r812, %r312, 2021161080;
	xor.b32  	%r325, %r812, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r316, %r317}, {%r318}, {%r319}, {%r284, %r285};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r322, %r323}, {%r324}, {%r325}, {%r290, %r291};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r328, %r329}, {%r318}, {%r325}, {%r302, %r303};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r334, %r335}, {%r324}, {%r319}, {%r328, %r329};
	// end inline asm
	sub.s32 	%r813, %r316, %r322;
	add.s32 	%r814, %r813, 2;
	shr.s32 	%r342, %r814, 2;
	sub.s32 	%r815, %r317, %r323;
	add.s32 	%r816, %r815, 2;
	shr.s32 	%r345, %r816, 2;
	add.s32 	%r817, %r334, 2;
	shr.s32 	%r341, %r817, 2;
	add.s32 	%r818, %r335, 2;
	shr.s32 	%r344, %r818, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r340, %r341, %r342;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r343, %r344, %r345;
	// end inline asm
	st.shared.u32 	[%rd11+4], %r340;
	st.shared.u32 	[%rd12+4], %r343;
	ld.shared.u32 	%r347, [%rd13+4];
	// begin inline asm
	lop3.b32 %r346, %r347, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r819, %r346, 2021161080;
	xor.b32  	%r357, %r819, -2139062144;
	shr.u32 	%r351, %r347, 4;
	// begin inline asm
	lop3.b32 %r350, %r351, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r820, %r350, 2021161080;
	xor.b32  	%r363, %r820, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r354, %r355}, {%r222}, {%r357}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r360, %r361}, {%r228}, {%r363}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r366, %r367}, {%r222}, {%r363}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r372, %r373}, {%r228}, {%r357}, {%r366, %r367};
	// end inline asm
	ld.shared.u32 	%r379, [%rd13+8];
	// begin inline asm
	lop3.b32 %r378, %r379, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r821, %r378, 2021161080;
	xor.b32  	%r389, %r821, -2139062144;
	shr.u32 	%r383, %r379, 4;
	// begin inline asm
	lop3.b32 %r382, %r383, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r822, %r382, 2021161080;
	xor.b32  	%r395, %r822, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r386, %r387}, {%r254}, {%r389}, {%r354, %r355};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r392, %r393}, {%r260}, {%r395}, {%r360, %r361};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r398, %r399}, {%r254}, {%r395}, {%r372, %r373};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r404, %r405}, {%r260}, {%r389}, {%r398, %r399};
	// end inline asm
	ld.shared.u32 	%r411, [%rd13+12];
	// begin inline asm
	lop3.b32 %r410, %r411, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r823, %r410, 2021161080;
	xor.b32  	%r421, %r823, -2139062144;
	shr.u32 	%r415, %r411, 4;
	// begin inline asm
	lop3.b32 %r414, %r415, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r824, %r414, 2021161080;
	xor.b32  	%r427, %r824, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r418, %r419}, {%r286}, {%r421}, {%r386, %r387};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r424, %r425}, {%r292}, {%r427}, {%r392, %r393};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r430, %r431}, {%r286}, {%r427}, {%r404, %r405};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r436, %r437}, {%r292}, {%r421}, {%r430, %r431};
	// end inline asm
	ld.shared.u32 	%r443, [%rd13+16];
	// begin inline asm
	lop3.b32 %r442, %r443, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r825, %r442, 2021161080;
	xor.b32  	%r453, %r825, -2139062144;
	shr.u32 	%r447, %r443, 4;
	// begin inline asm
	lop3.b32 %r446, %r447, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r826, %r446, 2021161080;
	xor.b32  	%r459, %r826, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r450, %r451}, {%r318}, {%r453}, {%r418, %r419};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r456, %r457}, {%r324}, {%r459}, {%r424, %r425};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r462, %r463}, {%r318}, {%r459}, {%r436, %r437};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r468, %r469}, {%r324}, {%r453}, {%r462, %r463};
	// end inline asm
	sub.s32 	%r827, %r450, %r456;
	add.s32 	%r828, %r827, 2;
	shr.s32 	%r476, %r828, 2;
	sub.s32 	%r829, %r451, %r457;
	add.s32 	%r830, %r829, 2;
	shr.s32 	%r479, %r830, 2;
	add.s32 	%r831, %r468, 2;
	shr.s32 	%r475, %r831, 2;
	add.s32 	%r832, %r469, 2;
	shr.s32 	%r478, %r832, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r474, %r475, %r476;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r477, %r478, %r479;
	// end inline asm
	st.shared.u32 	[%rd15+4], %r474;
	st.shared.u32 	[%rd16+4], %r477;
	ld.shared.u32 	%r481, [%rd17+4];
	// begin inline asm
	lop3.b32 %r480, %r481, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r833, %r480, 2021161080;
	xor.b32  	%r491, %r833, -2139062144;
	shr.u32 	%r485, %r481, 4;
	// begin inline asm
	lop3.b32 %r484, %r485, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r834, %r484, 2021161080;
	xor.b32  	%r497, %r834, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r488, %r489}, {%r222}, {%r491}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r494, %r495}, {%r228}, {%r497}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r500, %r501}, {%r222}, {%r497}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r506, %r507}, {%r228}, {%r491}, {%r500, %r501};
	// end inline asm
	ld.shared.u32 	%r513, [%rd17+8];
	// begin inline asm
	lop3.b32 %r512, %r513, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r835, %r512, 2021161080;
	xor.b32  	%r523, %r835, -2139062144;
	shr.u32 	%r517, %r513, 4;
	// begin inline asm
	lop3.b32 %r516, %r517, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r836, %r516, 2021161080;
	xor.b32  	%r529, %r836, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r520, %r521}, {%r254}, {%r523}, {%r488, %r489};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r526, %r527}, {%r260}, {%r529}, {%r494, %r495};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r532, %r533}, {%r254}, {%r529}, {%r506, %r507};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r538, %r539}, {%r260}, {%r523}, {%r532, %r533};
	// end inline asm
	ld.shared.u32 	%r545, [%rd17+12];
	// begin inline asm
	lop3.b32 %r544, %r545, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r837, %r544, 2021161080;
	xor.b32  	%r555, %r837, -2139062144;
	shr.u32 	%r549, %r545, 4;
	// begin inline asm
	lop3.b32 %r548, %r549, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r838, %r548, 2021161080;
	xor.b32  	%r561, %r838, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r552, %r553}, {%r286}, {%r555}, {%r520, %r521};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r558, %r559}, {%r292}, {%r561}, {%r526, %r527};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r564, %r565}, {%r286}, {%r561}, {%r538, %r539};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r570, %r571}, {%r292}, {%r555}, {%r564, %r565};
	// end inline asm
	ld.shared.u32 	%r577, [%rd17+16];
	// begin inline asm
	lop3.b32 %r576, %r577, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r839, %r576, 2021161080;
	xor.b32  	%r587, %r839, -2139062144;
	shr.u32 	%r581, %r577, 4;
	// begin inline asm
	lop3.b32 %r580, %r581, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r840, %r580, 2021161080;
	xor.b32  	%r593, %r840, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r584, %r585}, {%r318}, {%r587}, {%r552, %r553};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r590, %r591}, {%r324}, {%r593}, {%r558, %r559};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r596, %r597}, {%r318}, {%r593}, {%r570, %r571};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r602, %r603}, {%r324}, {%r587}, {%r596, %r597};
	// end inline asm
	sub.s32 	%r841, %r584, %r590;
	add.s32 	%r842, %r841, 2;
	shr.s32 	%r610, %r842, 2;
	sub.s32 	%r843, %r585, %r591;
	add.s32 	%r844, %r843, 2;
	shr.s32 	%r613, %r844, 2;
	add.s32 	%r845, %r602, 2;
	shr.s32 	%r609, %r845, 2;
	add.s32 	%r846, %r603, 2;
	shr.s32 	%r612, %r846, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r608, %r609, %r610;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r611, %r612, %r613;
	// end inline asm
	st.shared.u32 	[%rd19+4], %r608;
	st.shared.u32 	[%rd20+4], %r611;
	ld.shared.u32 	%r615, [%rd21+4];
	// begin inline asm
	lop3.b32 %r614, %r615, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r847, %r614, 2021161080;
	xor.b32  	%r625, %r847, -2139062144;
	shr.u32 	%r619, %r615, 4;
	// begin inline asm
	lop3.b32 %r618, %r619, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r848, %r618, 2021161080;
	xor.b32  	%r631, %r848, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r622, %r623}, {%r222}, {%r625}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r628, %r629}, {%r228}, {%r631}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r634, %r635}, {%r222}, {%r631}, {%r224, %r224};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r640, %r641}, {%r228}, {%r625}, {%r634, %r635};
	// end inline asm
	ld.shared.u32 	%r647, [%rd21+8];
	// begin inline asm
	lop3.b32 %r646, %r647, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r849, %r646, 2021161080;
	xor.b32  	%r657, %r849, -2139062144;
	shr.u32 	%r651, %r647, 4;
	// begin inline asm
	lop3.b32 %r650, %r651, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r850, %r650, 2021161080;
	xor.b32  	%r663, %r850, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r654, %r655}, {%r254}, {%r657}, {%r622, %r623};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r660, %r661}, {%r260}, {%r663}, {%r628, %r629};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r666, %r667}, {%r254}, {%r663}, {%r640, %r641};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r672, %r673}, {%r260}, {%r657}, {%r666, %r667};
	// end inline asm
	ld.shared.u32 	%r679, [%rd21+12];
	// begin inline asm
	lop3.b32 %r678, %r679, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r851, %r678, 2021161080;
	xor.b32  	%r689, %r851, -2139062144;
	shr.u32 	%r683, %r679, 4;
	// begin inline asm
	lop3.b32 %r682, %r683, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r852, %r682, 2021161080;
	xor.b32  	%r695, %r852, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r686, %r687}, {%r286}, {%r689}, {%r654, %r655};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r692, %r693}, {%r292}, {%r695}, {%r660, %r661};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r698, %r699}, {%r286}, {%r695}, {%r672, %r673};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r704, %r705}, {%r292}, {%r689}, {%r698, %r699};
	// end inline asm
	ld.shared.u32 	%r711, [%rd21+16];
	// begin inline asm
	lop3.b32 %r710, %r711, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r853, %r710, 2021161080;
	xor.b32  	%r721, %r853, -2139062144;
	shr.u32 	%r715, %r711, 4;
	// begin inline asm
	lop3.b32 %r714, %r715, %r214, %r215, 40;
	// end inline asm
	add.s32 	%r854, %r714, 2021161080;
	xor.b32  	%r727, %r854, -2139062144;
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r718, %r719}, {%r318}, {%r721}, {%r686, %r687};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r724, %r725}, {%r324}, {%r727}, {%r692, %r693};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r730, %r731}, {%r318}, {%r727}, {%r704, %r705};
	// end inline asm
	// begin inline asm
	mma.sync.aligned.m8n8k16.row.col.satfinite.s32.s8.s8.s32 {%r736, %r737}, {%r324}, {%r721}, {%r730, %r731};
	// end inline asm
	sub.s32 	%r855, %r718, %r724;
	add.s32 	%r856, %r855, 2;
	shr.s32 	%r744, %r856, 2;
	sub.s32 	%r857, %r719, %r725;
	add.s32 	%r858, %r857, 2;
	shr.s32 	%r747, %r858, 2;
	add.s32 	%r859, %r736, 2;
	shr.s32 	%r743, %r859, 2;
	add.s32 	%r860, %r737, 2;
	shr.s32 	%r746, %r860, 2;
	// begin inline asm
	cvt.pack.sat.s16.s32 %r742, %r743, %r744;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s16.s32 %r745, %r746, %r747;
	// end inline asm
	st.shared.u32 	[%rd23+4], %r742;
	st.shared.u32 	[%rd24+4], %r745;
	bar.sync 	0;
	ld.shared.u32 	%r861, [%rd25+4];
	ld.shared.u32 	%r862, [%rd26+4];
	ld.shared.u32 	%r863, [%rd27+4];
	ld.shared.u32 	%r864, [%rd28+4];
	ld.shared.u32 	%r865, [%rd29+2560];
	ld.shared.u32 	%r866, [%rd30+2560];
	ld.shared.u32 	%r867, [%rd31+2560];
	ld.shared.u32 	%r868, [%rd32+2560];
	cvt.s32.s16 	%r869, %r861;
	shr.s32 	%r870, %r861, 16;
	cvt.s32.s16 	%r871, %r862;
	shr.s32 	%r872, %r862, 16;
	cvt.s32.s16 	%r873, %r863;
	shr.s32 	%r874, %r863, 16;
	cvt.s32.s16 	%r875, %r864;
	shr.s32 	%r876, %r864, 16;
	cvt.s32.s16 	%r877, %r865;
	shr.s32 	%r878, %r865, 16;
	cvt.s32.s16 	%r879, %r866;
	shr.s32 	%r880, %r866, 16;
	cvt.s32.s16 	%r881, %r867;
	shr.s32 	%r882, %r867, 16;
	cvt.s32.s16 	%r883, %r868;
	shr.s32 	%r884, %r868, 16;
	add.s32 	%r885, %r869, %r15;
	shr.s32 	%r886, %r885, %r16;
	add.s32 	%r887, %r870, %r15;
	shr.s32 	%r888, %r887, %r16;
	add.s32 	%r889, %r871, %r15;
	shr.s32 	%r890, %r889, %r16;
	add.s32 	%r891, %r872, %r15;
	shr.s32 	%r892, %r891, %r16;
	add.s32 	%r893, %r873, %r15;
	shr.s32 	%r894, %r893, %r16;
	add.s32 	%r895, %r874, %r15;
	shr.s32 	%r896, %r895, %r16;
	add.s32 	%r897, %r875, %r15;
	shr.s32 	%r898, %r897, %r16;
	add.s32 	%r899, %r876, %r15;
	shr.s32 	%r900, %r899, %r16;
	add.s32 	%r901, %r877, %r17;
	shr.s32 	%r902, %r901, %r18;
	add.s32 	%r903, %r878, %r17;
	shr.s32 	%r904, %r903, %r18;
	add.s32 	%r905, %r879, %r17;
	shr.s32 	%r906, %r905, %r18;
	add.s32 	%r907, %r880, %r17;
	shr.s32 	%r908, %r907, %r18;
	add.s32 	%r909, %r881, %r17;
	shr.s32 	%r910, %r909, %r18;
	add.s32 	%r911, %r882, %r17;
	shr.s32 	%r912, %r911, %r18;
	add.s32 	%r913, %r883, %r17;
	shr.s32 	%r914, %r913, %r18;
	add.s32 	%r915, %r884, %r17;
	shr.s32 	%r916, %r915, %r18;
	max.s32 	%r917, %r886, -7;
	min.s32 	%r753, %r917, 7;
	max.s32 	%r918, %r888, -7;
	min.s32 	%r760, %r918, 7;
	max.s32 	%r919, %r890, -7;
	min.s32 	%r752, %r919, 7;
	max.s32 	%r920, %r892, -7;
	min.s32 	%r759, %r920, 7;
	max.s32 	%r921, %r894, -7;
	min.s32 	%r750, %r921, 7;
	max.s32 	%r922, %r896, -7;
	min.s32 	%r757, %r922, 7;
	max.s32 	%r923, %r898, -7;
	min.s32 	%r749, %r923, 7;
	max.s32 	%r924, %r900, -7;
	min.s32 	%r756, %r924, 7;
	max.s32 	%r925, %r902, -7;
	min.s32 	%r771, %r925, 7;
	max.s32 	%r926, %r904, -7;
	min.s32 	%r778, %r926, 7;
	max.s32 	%r927, %r906, -7;
	min.s32 	%r770, %r927, 7;
	max.s32 	%r928, %r908, -7;
	min.s32 	%r777, %r928, 7;
	max.s32 	%r929, %r910, -7;
	min.s32 	%r768, %r929, 7;
	max.s32 	%r930, %r912, -7;
	min.s32 	%r775, %r930, 7;
	max.s32 	%r931, %r914, -7;
	min.s32 	%r767, %r931, 7;
	max.s32 	%r932, %r916, -7;
	min.s32 	%r774, %r932, 7;
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r748, %r749, %r750, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r751, %r752, %r753, %r748;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r755, %r756, %r757, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r758, %r759, %r760, %r755;
	// end inline asm
	shl.b32 	%r763, %r758, 4;
	mov.u32 	%r765, -252645136;
	// begin inline asm
	lop3.b32 %r941, %r763, %r751, %r765, 228;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r766, %r767, %r768, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r769, %r770, %r771, %r766;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r773, %r774, %r775, 0;
	// end inline asm
	// begin inline asm
	cvt.pack.sat.s8.s32.b32 %r776, %r777, %r778, %r773;
	// end inline asm
	shl.b32 	%r781, %r776, 4;
	// begin inline asm
	lop3.b32 %r953, %r781, %r769, %r765, 228;
	// end inline asm
	setp.eq.s32 	%p8, %r1076, 0;
	selp.b32 	%r1087, %r941, %r1087, %p8;
	selp.b32 	%r1088, %r941, %r1088, %p8;
	setp.eq.s32 	%p9, %r1076, 4194304;
	selp.b32 	%r1085, %r941, %r1085, %p9;
	selp.b32 	%r1086, %r941, %r1086, %p9;
	setp.eq.s32 	%p10, %r1076, 8388608;
	selp.b32 	%r1083, %r941, %r1083, %p10;
	selp.b32 	%r1084, %r941, %r1084, %p10;
	selp.b32 	%r1081, %r953, %r1081, %p8;
	selp.b32 	%r1082, %r953, %r1082, %p8;
	selp.b32 	%r1079, %r953, %r1079, %p9;
	selp.b32 	%r1080, %r953, %r1080, %p9;
	selp.b32 	%r1077, %r953, %r1077, %p10;
	selp.b32 	%r1078, %r953, %r1078, %p10;
	add.s32 	%r1076, %r1076, 4194304;
	setp.ne.s32 	%p11, %r1076, 16777216;
	@%p11 bra 	LBB0_8;
// %bb.9:                               // %L6700
                                        //   in Loop: Header=BB0_7 Depth=1
	setp.eq.s32 	%p12, %r20, 0;
	setp.eq.s32 	%p13, %r19, 0;
	// begin inline asm
	prmt.b32 %r933, %r1087, %r1085, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r936, %r1088, %r1086, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r939, %r1083, %r941, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r942, %r1084, %r941, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r945, %r1081, %r1079, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r948, %r1082, %r1080, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r951, %r1077, %r953, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r954, %r1078, %r953, 29521;
	// end inline asm
	selp.b32 	%r1005, %r936, %r933, %p13;
	shfl.sync.bfly.b32	%r1006, %r1005, 1, 31, -1;
	selp.b32 	%r959, %r1006, %r936, %p13;
	selp.b32 	%r958, %r933, %r1006, %p13;
	selp.b32 	%r1007, %r942, %r939, %p13;
	shfl.sync.bfly.b32	%r1008, %r1007, 1, 31, -1;
	selp.b32 	%r965, %r1008, %r942, %p13;
	selp.b32 	%r964, %r939, %r1008, %p13;
	selp.b32 	%r1009, %r948, %r945, %p13;
	shfl.sync.bfly.b32	%r1010, %r1009, 1, 31, -1;
	selp.b32 	%r971, %r1010, %r948, %p13;
	selp.b32 	%r970, %r945, %r1010, %p13;
	selp.b32 	%r1011, %r954, %r951, %p13;
	shfl.sync.bfly.b32	%r1012, %r1011, 1, 31, -1;
	selp.b32 	%r977, %r1012, %r954, %p13;
	selp.b32 	%r976, %r951, %r1012, %p13;
	// begin inline asm
	prmt.b32 %r957, %r958, %r959, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r960, %r958, %r959, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r963, %r964, %r965, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r966, %r964, %r965, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r969, %r970, %r971, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r972, %r970, %r971, 29521;
	// end inline asm
	// begin inline asm
	prmt.b32 %r975, %r976, %r977, 25152;
	// end inline asm
	// begin inline asm
	prmt.b32 %r978, %r976, %r977, 29521;
	// end inline asm
	selp.b32 	%r1013, %r963, %r957, %p12;
	shfl.sync.bfly.b32	%r1014, %r1013, 2, 31, -1;
	selp.b32 	%r983, %r1014, %r963, %p12;
	selp.b32 	%r982, %r957, %r1014, %p12;
	selp.b32 	%r1015, %r966, %r960, %p12;
	shfl.sync.bfly.b32	%r1016, %r1015, 2, 31, -1;
	selp.b32 	%r989, %r1016, %r966, %p12;
	selp.b32 	%r988, %r960, %r1016, %p12;
	selp.b32 	%r1017, %r975, %r969, %p12;
	shfl.sync.bfly.b32	%r1018, %r1017, 2, 31, -1;
	selp.b32 	%r995, %r1018, %r975, %p12;
	selp.b32 	%r994, %r969, %r1018, %p12;
	selp.b32 	%r1019, %r978, %r972, %p12;
	shfl.sync.bfly.b32	%r1020, %r1019, 2, 31, -1;
	selp.b32 	%r1001, %r1020, %r978, %p12;
	selp.b32 	%r1000, %r972, %r1020, %p12;
	// begin inline asm
	prmt.b32 %r981, %r982, %r983, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r984, %r982, %r983, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r987, %r988, %r989, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r990, %r988, %r989, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r993, %r994, %r995, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r996, %r994, %r995, 30258;
	// end inline asm
	// begin inline asm
	prmt.b32 %r999, %r1000, %r1001, 21520;
	// end inline asm
	// begin inline asm
	prmt.b32 %r1002, %r1000, %r1001, 30258;
	// end inline asm
	selp.b32 	%r1021, %r987, %r981, %p5;
	shfl.sync.bfly.b32	%r1022, %r1021, 4, 31, -1;
	selp.b32 	%r1023, %r1022, %r987, %p5;
	selp.b32 	%r1024, %r981, %r1022, %p5;
	selp.b32 	%r1025, %r990, %r984, %p5;
	shfl.sync.bfly.b32	%r1026, %r1025, 4, 31, -1;
	selp.b32 	%r1027, %r1026, %r990, %p5;
	selp.b32 	%r1028, %r984, %r1026, %p5;
	selp.b32 	%r1029, %r999, %r993, %p5;
	shfl.sync.bfly.b32	%r1030, %r1029, 4, 31, -1;
	selp.b32 	%r1031, %r1030, %r999, %p5;
	selp.b32 	%r1032, %r993, %r1030, %p5;
	selp.b32 	%r1033, %r1002, %r996, %p5;
	shfl.sync.bfly.b32	%r1034, %r1033, 4, 31, -1;
	selp.b32 	%r1035, %r1034, %r1002, %p5;
	selp.b32 	%r1036, %r996, %r1034, %p5;
	selp.b32 	%r1037, %r1028, %r1024, %p13;
	shfl.sync.bfly.b32	%r1038, %r1037, 1, 31, -1;
	selp.b32 	%r1039, %r1038, %r1028, %p13;
	selp.b32 	%r1040, %r1024, %r1038, %p13;
	selp.b32 	%r1041, %r1027, %r1023, %p13;
	shfl.sync.bfly.b32	%r1042, %r1041, 1, 31, -1;
	selp.b32 	%r1043, %r1042, %r1027, %p13;
	selp.b32 	%r1044, %r1023, %r1042, %p13;
	selp.b32 	%r1045, %r1036, %r1032, %p13;
	shfl.sync.bfly.b32	%r1046, %r1045, 1, 31, -1;
	selp.b32 	%r1047, %r1046, %r1036, %p13;
	selp.b32 	%r1048, %r1032, %r1046, %p13;
	selp.b32 	%r1049, %r1035, %r1031, %p13;
	shfl.sync.bfly.b32	%r1050, %r1049, 1, 31, -1;
	selp.b32 	%r1051, %r1050, %r1035, %p13;
	selp.b32 	%r1052, %r1031, %r1050, %p13;
	shl.b32 	%r1053, %r1063, 7;
	or.b32  	%r1054, %r1053, %r21;
	or.b32  	%r1055, %r1054, %r22;
	or.b32  	%r1056, %r1055, %r23;
	or.b32  	%r1057, %r1056, %r26;
	or.b32  	%r1058, %r1057, %r24;
	or.b32  	%r1059, %r1058, %r25;
	or.b32  	%r1060, %r1059, 4;
	cvt.u64.u32 	%rd128, %r1060;
	add.s64 	%rd129, %rd4, %rd128;
	st.global.v4.u32 	[%rd129+-4], {%r1040, %r1044, %r1039, %r1043};
	or.b32  	%r1061, %r1059, 32772;
	cvt.u64.u32 	%rd130, %r1061;
	add.s64 	%rd131, %rd4, %rd130;
	st.global.v4.u32 	[%rd131+-4], {%r1048, %r1052, %r1047, %r1051};
	add.s32 	%r69, %r1063, 1;
	add.s32 	%r1062, %r1062, 16777216;
	setp.ne.s32 	%p15, %r1063, 255;
	mov.u32 	%r1063, %r69;
	@%p15 bra 	LBB0_7;
// %bb.10:                              // %L7264
	ret;
LBB0_1:                                 // %L9
	mov.u64 	%rd34, exception1;
	cvta.global.u64 	%rd35, %rd34;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd35;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 0
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 1
	// begin inline asm
	exit;
	// end inline asm
LBB0_12:                                // %L7268
	mov.u64 	%rd134, exception1;
	cvta.global.u64 	%rd135, %rd134;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd135;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 6
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 7
	// begin inline asm
	exit;
	// end inline asm
LBB0_4:                                 // %L34
	mov.u64 	%rd40, exception1;
	cvta.global.u64 	%rd41, %rd40;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd41;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 2
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 3
	// begin inline asm
	exit;
	// end inline asm
LBB0_11:                                // %L7265
	mov.u64 	%rd132, exception1;
	cvta.global.u64 	%rd133, %rd132;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd133;
	call.uni 
	gpu_report_exception, 
	(
	param0
	);
	} // callseq 4
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .align 8 .b8 param0[8];
	st.param.b64 	[param0+0], %rd33;
	call.uni 
	gpu_signal_exception, 
	(
	param0
	);
	} // callseq 5
	// begin inline asm
	exit;
	// end inline asm
                                        // -- End function
}
